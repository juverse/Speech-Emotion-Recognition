{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross lingual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "df_urdu = pd.read_csv(r'..\\Data\\Urdu\\features_urdu.csv')\n",
    "df_english = pd.read_csv(r'..\\Data\\English\\features_english.csv')\n",
    "df_german = pd.read_csv(r'..\\Data\\German\\features_german.csv')\n",
    "df_italian = pd.read_csv(r'..\\Data\\Italian\\features_italian.csv')\n",
    "\n",
    "df_western = pd.concat([df_german, df_english, df_italian], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_decile_test_data_to_train_data(df_train, df_test, decile: int = 0):\n",
    "    if not (0 <= decile <= 8):\n",
    "        return ValueError(\"Value Error: Input out of range\")\n",
    "    else:\n",
    "        # Split up test data into 10 deciles for gradual increase \n",
    "        df_test_parts = np.array_split(df_test, 10)\n",
    "        \n",
    "        # 1 to 8 deciles (= 10-80%) shall be used for training\n",
    "        if decile == 0:\n",
    "            df_test_for_training = df_test_parts[decile]\n",
    "        else:\n",
    "            df_test_for_training = pd.concat([df for df in df_test_parts[:decile]])\n",
    "    \n",
    "        df_train = pd.concat([df_train, df_test_for_training], ignore_index=True)\n",
    "\n",
    "        # remove used data from test data\n",
    "        df_test = pd.concat([df for df in df_test_parts[decile:]], ignore_index=True)\n",
    "\n",
    "        return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "C:\\Users\\david\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\core.py:158: UserWarning: [16:33:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Define train and test dfs\n",
    "df_train, df_test = add_decile_test_data_to_train_data(df_train=df_urdu, df_test=df_italian, decile=7)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = df_train.drop([\"emotion\", \"speaker_id\", \"filename\", \"valence\"], axis=1) # Features\n",
    "y_train = df_train['valence']\n",
    "\n",
    "X_test = df_test.drop([\"emotion\", \"speaker_id\", \"filename\", \"valence\"], axis=1) # Features\n",
    "y_test = df_test['valence']\n",
    "\n",
    "# Define models for comparison\n",
    "models = {\n",
    "    'SVM (linear)': SVC(kernel='linear', C=1.0, random_state=42, probability=True),\n",
    "    'SVM (rbf)': SVC(kernel='rbf', C=1.0, random_state=42, probability=True),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results = []\n",
    "\n",
    "# Compare different models\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Define pipeline: Scaling + Model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', model)\n",
    "        ])\n",
    "   \n",
    "    # Fit the model on training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    #y_prob = pipeline.predict_proba(X_test) if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Metrics calculation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "  \n",
    "    # Save results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Balanced Accuracy': balanced_accuracy,\n",
    "        'Precision (Weighted)': precision,\n",
    "        'Recall (Weighted)': recall,\n",
    "        'F1-Score (Weighted)': f1\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add random baseline/dummy\n",
    "# Generate random predictions from the existing labels\n",
    "y_random = np.random.choice(y_train.unique(), size=len(y_test), replace=True)\n",
    "\n",
    "# Metrics for Random Baseline\n",
    "random_accuracy = accuracy_score(y_test, y_random)\n",
    "random_balanced_accuracy = balanced_accuracy_score(y_test, y_random)\n",
    "random_precision = precision_score(y_test, y_random, average='weighted', zero_division=0)\n",
    "random_recall = recall_score(y_test, y_random, average='weighted')\n",
    "random_f1 = f1_score(y_test, y_random, average='weighted')\n",
    "\n",
    "# Save Random Baseline results\n",
    "results.append({\n",
    "    'Model': 'stratified Dummy',\n",
    "    'Accuracy': random_accuracy,\n",
    "    'Balanced Accuracy': random_balanced_accuracy,\n",
    "    'Precision (Weighted)': random_precision,\n",
    "    'Recall (Weighted)': random_recall,\n",
    "    'F1-Score (Weighted)': random_f1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision (Weighted)</th>\n",
       "      <th>Recall (Weighted)</th>\n",
       "      <th>F1-Score (Weighted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM (linear)</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.576531</td>\n",
       "      <td>0.616571</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.572857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.576531</td>\n",
       "      <td>0.616571</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.572857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.605714</td>\n",
       "      <td>0.575603</td>\n",
       "      <td>0.602957</td>\n",
       "      <td>0.605714</td>\n",
       "      <td>0.577601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.557978</td>\n",
       "      <td>0.613867</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.539975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM (rbf)</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.543599</td>\n",
       "      <td>0.599654</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.516735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stratified Dummy</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.537570</td>\n",
       "      <td>0.544146</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.543431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Balanced Accuracy  Precision (Weighted)  \\\n",
       "0         SVM (linear)  0.611429           0.576531              0.616571   \n",
       "2  Logistic Regression  0.611429           0.576531              0.616571   \n",
       "4    Gradient Boosting  0.605714           0.575603              0.602957   \n",
       "3        Random Forest  0.600000           0.557978              0.613867   \n",
       "1            SVM (rbf)  0.588571           0.543599              0.599654   \n",
       "5     stratified Dummy  0.542857           0.537570              0.544146   \n",
       "\n",
       "   Recall (Weighted)  F1-Score (Weighted)  \n",
       "0           0.611429             0.572857  \n",
       "2           0.611429             0.572857  \n",
       "4           0.605714             0.577601  \n",
       "3           0.600000             0.539975  \n",
       "1           0.588571             0.516735  \n",
       "5           0.542857             0.543431  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame(results).sort_values(by='Balanced Accuracy', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data as csv\n",
    "results_df.to_csv(\"../Evaluation/Percentage/train_urdu_test_italian_80percent.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
